# .gitlab-ci.yml
# This file defines the CI/CD pipeline for your Python web application.

# Define the stages of the pipeline
stages:
  - lint                # Code quality and style checks
  - test                # Unit tests
  - integration_test    # Integration tests (e.g., database connectivity)
  - dependency_scan     # Check for vulnerabilities in dependencies
  - sast_scan           # Static Application Security Testing
  - secret_detection_scan # Scan for hardcoded secrets
  - build_docker        # Build Docker image
  - image_scan          # Scan Docker image for vulnerabilities
  - deploy_staging      # Deploy to the staging environment
  - dast_scan           # Dynamic Application Security Testing

# Default settings for all jobs
default:
  cache:
    key: "$CI_COMMIT_REF_SLUG-pip"
    paths:
      - .cache/pip/
      - venv/
    policy: pull-push

variables:
  PYTHON_VERSION: "3.8"
  REQUIREMENTS_FILE: "requirements.txt"
  APP_CODE_PATH: "app/"
  TEST_FILES_PATH: "tests/"
  DOCKERFILE_PATH: "Dockerfile"         # Assumes Dockerfile is in the root
  DOCKER_CONTEXT_PATH: "."              # Docker build context is the root
  DOCKER_IMAGE_NAME: "$CI_REGISTRY_IMAGE:$CI_COMMIT_SHA"
  STAGING_SERVER_IP: "146.59.45.19"
  STAGING_SERVER_USER: "ubuntu"
  STAGING_SERVER_SSH_PORT: "2222"
  STAGING_APP_URL: "http://test.dot.arifjan.su" # Your staging domain
  STAGING_DEPLOY_PATH: "/home/ubuntu/book_catalog_staging" # Deployment path on staging server
  ZAP_REPORT_JSON: "zap-report.json"
  ZAP_REPORT_HTML: "zap-report.html"
  # STAGING_SERVER_PASSWORD will be set as a CI/CD variable in GitLab

# --- LINTING STAGE ---
lint_code:
  stage: lint
  image: python:${PYTHON_VERSION}
  before_script:
    - python -m venv venv
    - source venv/bin/activate
    - pip config set global.cache-dir "$(pwd)/.cache/pip"
    - pip install flake8
  script:
    - echo "Linting code with Flake8..."
    - flake8 ${APP_CODE_PATH}
  allow_failure: false

# --- UNIT TESTING STAGE ---
test_application:
  stage: test
  image: python:${PYTHON_VERSION}
  before_script:
    - python -m venv venv
    - source venv/bin/activate
    - pip config set global.cache-dir "$(pwd)/.cache/pip"
    - pip install -r ${REQUIREMENTS_FILE}
    - pip install pytest
  script:
    - echo "Running unit tests..."
    - python -m pytest ${TEST_FILES_PATH}test_app.py
  allow_failure: false

# --- INTEGRATION TESTING STAGE ---
test_database_integration:
  stage: integration_test
  image: python:${PYTHON_VERSION}
  services:
    - name: postgres:13
      alias: test-postgres-db
  variables:
    SQLALCHEMY_DATABASE_URI: "postgresql://runner:@test-postgres-db:5432/testdb"
    POSTGRES_USER: "runner"
    POSTGRES_DB: "testdb"
    POSTGRES_HOST_AUTH_METHOD: "trust"
  before_script:
    - python -m venv venv
    - source venv/bin/activate
    - pip config set global.cache-dir "$(pwd)/.cache/pip"
    - pip install -r ${REQUIREMENTS_FILE}
    - pip install pytest psycopg2-binary
  script:
    - echo "Running database integration tests..."
    - python -m pytest ${TEST_FILES_PATH}test_integration_database.py
  allow_failure: false

# --- DEPENDENCY SCANNING STAGE ---
scan_dependencies:
  stage: dependency_scan
  image: python:${PYTHON_VERSION}
  before_script:
    - python -m venv venv
    - source venv/bin/activate
    - pip config set global.cache-dir "$(pwd)/.cache/pip"
    - pip install safety
  script:
    - echo "Scanning dependencies for vulnerabilities with Safety..."
    - safety check -r ${REQUIREMENTS_FILE} --output json > safety-report.json || true
    - safety check -r ${REQUIREMENTS_FILE}
  artifacts:
    paths:
      - safety-report.json
    when: always
  allow_failure: false

# --- STATIC APPLICATION SECURITY TESTING (SAST) STAGE ---
static_analysis_bandit:
  stage: sast_scan
  image: python:${PYTHON_VERSION}
  before_script:
    - python -m venv venv
    - source venv/bin/activate
    - pip config set global.cache-dir "$(pwd)/.cache/pip"
    - pip install bandit
  script:
    - echo "Running SAST with Bandit..."
    - bandit -r ${APP_CODE_PATH} -f json -o bandit-report.json -ll -ii || if [ $? -eq 1 ]; then echo 'Bandit found issues!'; exit 1; else echo 'Bandit scan clear.'; exit 0; fi
  artifacts:
    paths:
      - bandit-report.json
    reports:
      sast: bandit-report.json
    when: always
  allow_failure: false

# --- SECRET DETECTION STAGE ---
scan_for_secrets:
  stage: secret_detection_scan
  image:
    name: zricethezav/gitleaks:latest
    entrypoint: [""]
  script:
    - echo "Scanning for secrets with Gitleaks..."
    - gitleaks detect --source . --report-path gitleaks-report.json --report-format json -v
  artifacts:
    paths:
      - gitleaks-report.json
    reports:
      secret_detection: gitleaks-report.json
    when: always
  allow_failure: false

# --- DOCKER BUILD STAGE ---
build_docker_image:
  stage: build_docker
  image: docker:latest
  services:
    - name: docker:dind
      alias: docker
  variables:
    DOCKER_HOST: tcp://docker:2375
    DOCKER_TLS_CERTDIR: ""
    DOCKER_DRIVER: overlay2
  before_script: |
    echo "Current CI_REGISTRY is: $CI_REGISTRY"
    echo "Current CI_REGISTRY_USER is: $CI_REGISTRY_USER"
    echo "Waiting for Docker daemon..."
    apk add --no-cache curl
    max_attempts=30
    attempt_num=0
    until curl --silent --fail http://docker:2375/_ping; do
      if [ ${attempt_num} -eq ${max_attempts} ]; then
        echo "Docker daemon did not start in time."
        exit 1
      fi
      attempt_num=$((attempt_num+1))
      echo "Waiting for docker daemon... attempt $attempt_num"
      sleep 2
    done
    echo "Docker daemon is ready."
    if [ -z "$CI_REGISTRY" ]; then echo "ERROR: CI_REGISTRY is not set!"; exit 1; fi
    echo "Logging in to GitLab Container Registry: $CI_REGISTRY"
    docker login -u "$CI_REGISTRY_USER" -p "$CI_JOB_TOKEN" "$CI_REGISTRY"
  script:
    - echo "Building Docker image from ${DOCKERFILE_PATH} with context ${DOCKER_CONTEXT_PATH}..."
    - docker build -t ${DOCKER_IMAGE_NAME} -f ${DOCKERFILE_PATH} ${DOCKER_CONTEXT_PATH}
    - echo "Pushing Docker image ${DOCKER_IMAGE_NAME} to GitLab Container Registry..."
    - docker push ${DOCKER_IMAGE_NAME}
  rules:
    - if: '$CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH || $CI_COMMIT_TAG'

# --- DOCKER IMAGE SCANNING STAGE ---
scan_docker_image:
  stage: image_scan
  image:
    name: aquasec/trivy:0.47.0 # Pinned to a version known to have /contrib/gitlab.tpl
    entrypoint: [""] # Override entrypoint to use shell
  variables:
    TRIVY_EXIT_CODE: "1" # Fail if vulnerabilities of specified severity are found
    TRIVY_SEVERITY: "CRITICAL" # Only fail on CRITICAL vulnerabilities
    CI: "true"
    GIT_STRATEGY: none # No need to clone the repo for this job
    TRIVY_USERNAME: "$CI_REGISTRY_USER" # For Trivy to auth with GitLab Registry
    TRIVY_PASSWORD: "$CI_JOB_TOKEN"     # For Trivy to auth with GitLab Registry
    TRIVY_NO_PROGRESS: "true"           # Suppress progress bar in logs
    TRIVY_CACHE_DIR: ".trivycache/"     # Define cache directory for Trivy DB
    FULL_IMAGE_NAME: "$DOCKER_IMAGE_NAME" # Use the image built in previous stage
  cache: # Cache Trivy's vulnerability database
    key: trivy-db
    paths:
      - .trivycache/
  before_script:
    - echo "Preparing for Trivy scan..."
    - trivy --version # Print Trivy version for debugging
    # Download/update the vulnerability database to the cache directory
    - time trivy image --download-db-only --cache-dir "$TRIVY_CACHE_DIR"
  script:
    - echo "Scanning Docker image ${FULL_IMAGE_NAME} from registry ${CI_REGISTRY} with Trivy..."
    - time trivy image --exit-code ${TRIVY_EXIT_CODE} --severity ${TRIVY_SEVERITY} --format template --template "@/contrib/gitlab.tpl" --output "gl-container-scanning-report.json" --cache-dir "$TRIVY_CACHE_DIR" "${FULL_IMAGE_NAME}"
    - echo "Trivy scan summary (High and Critical) - Job will fail if CRITICAL vulnerabilities are found:"
    - time trivy image --exit-code 0 --severity HIGH,CRITICAL --cache-dir "$TRIVY_CACHE_DIR" "${FULL_IMAGE_NAME}"
  artifacts:
    reports:
      container_scanning: gl-container-scanning-report.json
    paths:
      - gl-container-scanning-report.json
    when: always
  allow_failure: true
  needs: ["build_docker_image"]
  rules:
    - if: '$CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH || $CI_COMMIT_TAG'

# --- DEPLOY TO STAGING STAGE ---
deploy_to_staging:
  stage: deploy_staging
  image: alpine
  before_script:
    - apk add --no-cache sshpass openssh-client rsync
    - mkdir -p ~/.ssh
    - chmod 700 ~/.ssh
    - echo "$SSH_KNOWN_HOSTS" > ~/.ssh/known_hosts
    - chmod 644 ~/.ssh/known_hosts
  script: |
    echo "Current working directory: $(pwd)"
    echo "Listing files in current directory:"
    ls -la
    echo "Starting deployment to staging server ${STAGING_SERVER_IP} on port ${STAGING_SERVER_SSH_PORT}..."
    sed "s|\${DOCKER_IMAGE_TO_DEPLOY}|${DOCKER_IMAGE_NAME}|g" docker-compose.staging.yml > docker-compose.staging.processed.yml
    sshpass -p "$STAGING_SERVER_PASSWORD" scp -P ${STAGING_SERVER_SSH_PORT} -o StrictHostKeyChecking=no docker-compose.staging.processed.yml ${STAGING_SERVER_USER}@${STAGING_SERVER_IP}:${STAGING_DEPLOY_PATH}/docker-compose.yml
    sshpass -p "$STAGING_SERVER_PASSWORD" ssh -p ${STAGING_SERVER_SSH_PORT} -o StrictHostKeyChecking=no ${STAGING_SERVER_USER}@${STAGING_SERVER_IP} "
      echo 'Connected to staging server.';
      set -e;

      echo 'Updating package list and installing Docker and Docker Compose if not present...';
      if ! command -v docker &> /dev/null; then
        sudo apt-get update -y;
        sudo apt-get install -y docker.io;
        sudo systemctl start docker;
        sudo systemctl enable docker;
        echo 'Docker installed.';
      else
        echo 'Docker is already installed.';
      fi;

      if ! command -v docker-compose &> /dev/null; then
        sudo apt-get install -y docker-compose;
        echo 'Docker Compose installed.';
      else
        echo 'Docker Compose is already installed.';
      fi;

      echo 'Creating deployment directory ${STAGING_DEPLOY_PATH} if it does not exist...';
      mkdir -p ${STAGING_DEPLOY_PATH};
      cd ${STAGING_DEPLOY_PATH};

      echo 'Logging in to GitLab Container Registry on staging server...';
      sudo docker login -u \"${STAGING_REGISTRY_USER}\" -p \"${STAGING_REGISTRY_PASSWORD}\" ${CI_REGISTRY};

      echo 'Pulling the latest application image...';
      sudo docker-compose -f docker-compose.yml pull app;

      echo 'Stopping and removing old service container (if exists)...';
      sudo docker-compose -f docker-compose.yml stop app || true;
      sudo docker-compose -f docker-compose.yml rm -f app || true;

      echo 'Starting new application container...';
      sudo docker-compose -f docker-compose.yml up -d --force-recreate --remove-orphans app;

      echo 'Cleaning up unused Docker images...';
      sudo docker image prune -af;

      echo 'Deployment to ${STAGING_APP_URL} completed successfully.';
    "
  environment:
    name: staging
    url: ${STAGING_APP_URL}
  rules:
    - if: '$CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH'
  needs: ["build_docker_image", "scan_docker_image"]

# --- DYNAMIC APPLICATION SECURITY TESTING (DAST) STAGE ---
dynamic_analysis_zap:
  stage: dast_scan
  image: zaproxy/zap-stable # UPDATED ZAP IMAGE
  variables:
    ZAP_TARGET_URL: ${STAGING_APP_URL}
  script:
    - echo "Starting DAST scan with OWASP ZAP on ${ZAP_TARGET_URL}"
    - mkdir -p zap_reports
    - zap-baseline.py -t ${ZAP_TARGET_URL} -J zap_reports/${ZAP_REPORT_JSON} -r zap_reports/${ZAP_REPORT_HTML} || true
    - echo "DAST scan completed. Reports are in zap_reports/ directory."
  artifacts:
    paths:
      - zap_reports/${ZAP_REPORT_JSON}
      - zap_reports/${ZAP_REPORT_HTML}
    when: always
  allow_failure: true
  needs: ["deploy_to_staging"]
  rules:
    - if: '$CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH'

